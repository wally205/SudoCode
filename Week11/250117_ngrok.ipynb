{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3APVTQyPBB8",
        "outputId": "5919d328-5bb6-4fcd-be6e-de9cf1a95bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.118.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 35bzLC1iSNO3dLdUNL5gSRQM4Do_2DSrmxDU4ETur3tb8CziU\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6Ybj1GPRjMV",
        "outputId": "6c3ef7eb-5cea-42fc-8857-cd1b4e3470cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_size=2192, num_classes=102,\n",
        "                 d_model=256, nhead=4, num_layers=2\n",
        "                 , dim_feedforward=256, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_size, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = nn.Parameter(torch.randn(1, 60, d_model))\n",
        "        self.dropout_pos = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model,128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)                   # (B, T, d_model)\n",
        "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
        "        x = self.dropout_pos(x)\n",
        "        x = self.transformer(x)                  # (B, T, d_model)\n",
        "        out = x.mean(dim=1)                      # Mean pooling\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# ---------------- LOAD MODEL ----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerClassifier(input_size=2192, num_classes=102).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/256_256_128_102class_0.62.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qllZ0hxjQEJ6",
        "outputId": "ed633d04-38c3-4127-97b3-9abff8b4aadd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (input_proj): Sequential(\n",
              "    (0): Linear(in_features=2192, out_features=256, bias=True)\n",
              "    (1): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (dropout_pos): Dropout(p=0.3, inplace=False)\n",
              "  (transformer): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=102, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# state = torch.load(\"/content/256_256_128_2layers_0.78acc.pth\")\n",
        "\n",
        "# for k, v in state.items():\n",
        "#     print(k, v.shape)\n"
      ],
      "metadata": {
        "id": "mIA-ot06yjxX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mediapipe"
      ],
      "metadata": {
        "id": "emlN_IfZPLCQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6227420-63f7-45ec-cd3d-8a4bba183db7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.2/81.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.26.4 --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "O5j9huN01_GK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import mediapipe as mp\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import base64\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# ---------------- Mediapipe setup ----------------\n",
        "mp_holistic = mp.solutions.holistic\n",
        "POSE_LANDMARKS = [\n",
        "    mp.solutions.holistic.PoseLandmark.LEFT_SHOULDER,\n",
        "    mp.solutions.holistic.PoseLandmark.LEFT_ELBOW,\n",
        "    mp.solutions.holistic.PoseLandmark.LEFT_WRIST,\n",
        "    mp.solutions.holistic.PoseLandmark.RIGHT_SHOULDER,\n",
        "    mp.solutions.holistic.PoseLandmark.RIGHT_ELBOW,\n",
        "    mp.solutions.holistic.PoseLandmark.RIGHT_WRIST,\n",
        "]\n",
        "N_POSE_LANDMARKS = len(POSE_LANDMARKS)\n",
        "N_HAND_LANDMARKS = 21\n",
        "N_TOTAL_LANDMARKS = N_POSE_LANDMARKS + 2 * N_HAND_LANDMARKS  # 48\n",
        "POSE_FEATURES = N_TOTAL_LANDMARKS * 3  # 144\n",
        "\n",
        "# ---------------- ResNet setup ----------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
        "resnet.eval()\n",
        "\n",
        "resnet_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ---------------- Feature extraction ----------------\n",
        "def extract_mediapipe_features_from_frame(frame):\n",
        "    with mp_holistic.Holistic(static_image_mode=True) as holistic:\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = holistic.process(frame_rgb)\n",
        "\n",
        "    pose_kps = np.zeros((N_POSE_LANDMARKS, 3))\n",
        "    left_kps = np.zeros((N_HAND_LANDMARKS, 3))\n",
        "    right_kps = np.zeros((N_HAND_LANDMARKS, 3))\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        for i, lm_id in enumerate(POSE_LANDMARKS):\n",
        "            lm = results.pose_landmarks.landmark[lm_id]\n",
        "            pose_kps[i] = [lm.x, lm.y, lm.z]\n",
        "\n",
        "    if results.left_hand_landmarks:\n",
        "        left_kps = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark])\n",
        "\n",
        "    if results.right_hand_landmarks:\n",
        "        right_kps = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark])\n",
        "\n",
        "    return np.concatenate([pose_kps, left_kps, right_kps]).flatten()\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_resnet_features_from_frame(frame):\n",
        "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    x = resnet_transform(img_rgb).unsqueeze(0).to(device)\n",
        "    feat = resnet(x).squeeze().cpu().numpy()\n",
        "    return feat\n",
        "\n",
        "def extract_full_features(frame):\n",
        "    kps = extract_mediapipe_features_from_frame(frame)\n",
        "    cnn_feat = extract_resnet_features_from_frame(frame)\n",
        "    return np.concatenate([kps, cnn_feat])  # 2192 vector\n",
        "\n"
      ],
      "metadata": {
        "id": "RxbMvUZO1zWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09c4315-652e-4a23-f735-5cae614c7923"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jaxlib/plugin_support.py:71: RuntimeWarning: JAX plugin jax_cuda12_plugin version 0.7.2 is installed, but it is not compatible with the installed jaxlib version 0.7.1, so it will not be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 149MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actions list\n",
        "# ---------------- Base64 -> frame ----------------\n",
        "def decode_base64_to_frame(base64_str):\n",
        "    img_bytes = base64.b64decode(base64_str)\n",
        "    np_arr = np.frombuffer(img_bytes, np.uint8)\n",
        "    frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
        "    return frame\n",
        "\n",
        "def interpolate_keypoints(keypoints_sequence, target_len=60):\n",
        "    if len(keypoints_sequence) == 0:\n",
        "        return None\n",
        "\n",
        "    original_times = np.linspace(0, 1, len(keypoints_sequence))\n",
        "    target_times = np.linspace(0, 1, target_len)\n",
        "    num_features = keypoints_sequence[0].shape[0]\n",
        "\n",
        "    interpolated_sequence = np.zeros((target_len, num_features))\n",
        "    for i in range(num_features):\n",
        "        feature_values = [frame[i] for frame in keypoints_sequence]\n",
        "        interpolator = interp1d(original_times, feature_values,\n",
        "                                kind='cubic', bounds_error=False,\n",
        "                                fill_value='extrapolate')\n",
        "        interpolated_sequence[:, i] = interpolator(target_times)\n",
        "\n",
        "    return interpolated_sequence"
      ],
      "metadata": {
        "id": "Lhlg90uqbzBA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# -----------------------\n",
        "# Class mapping t·ª´ l√∫c train\n",
        "# -----------------------\n",
        "class_mapping = {'0': 0, '1': 1, '10': 2, '11': 3, '12': 4, '13': 5, '14': 6, '15': 7, '16': 8, '17': 9, '18': 10, '19': 11, '2': 12, '20': 13, '3': 14, '4': 15, '5': 16, '6': 17, '7': 18, '8': 19, '9': 20, 'a': 21, 'anh_trai': 22, 'b': 23, 'bao_nhi√™u': 24, 'bu·ªïi_chi·ªÅu': 25, 'bu·ªïi_s√°ng': 26, 'bu·ªïi_tr∆∞a': 27, 'bu·ªïi_t·ªëi': 28, 'b√†': 29, 'b√°c_sƒ©': 30, 'b·∫°n': 31, 'c': 32, 'cha': 33, 'ch√†o': 34, 'ch·ªã_g√°i': 35, 'ch·ªìng': 36, 'c√≤n_b·∫°n': 37, 'c√¥ng_an': 38, 'c√¥ng_nh√¢n': 39, 'c·∫£m_∆°n': 40, 'c·ªßa_b·∫°n': 41, 'c·ªßa_t√¥i': 42, 'd': 43, 'dd': 44, 'd·∫•u_m√≥c': 45, 'd·∫•u_m≈©': 46, 'd·∫•u_ƒÉ': 47, 'e': 48, 'ee': 49, 'em_g√°i': 50, 'em_trai': 51, 'g': 52, 'gia_ƒë√¨nh': 53, 'gi√°o_vi√™n': 54, 'h': 55, 'i': 56, 'k': 57, 'kh√¥ng': 58, 'k·ªπ_s∆∞': 59, 'l': 60, 'm': 61, 'm√†u_cam': 62, 'm√†u_h·ªìng': 63, 'm√†u_n√¢u': 64, 'm√†u_s·∫Øc': 65, 'm√†u_tr·∫Øng': 66, 'm√†u_t√≠m': 67, 'm√†u_xanh': 68, 'm√†u_ƒëen': 69, 'm√†u_ƒë·ªè': 70, 'm·∫π': 71, 'n': 72, 'ngh·ªÅ_nghi·ªáp': 73, 'nh√†': 74, 'o': 75, 'p': 76, 'phi√™n_d·ªãch': 77, 'q': 78, 'r': 79, 's': 80, 't': 81, 'th√≠ch': 82, 'tu·ªïi': 83, 't√™n': 84, 't√¥i': 85, 't·∫°m_bi·ªát': 86, 'u': 87, 'v': 88, 'v·ª£': 89, 'x': 90, 'xin_l·ªói': 91, 'y': 92, 'y_t√°': 93, '√¢': 94, '√¥': 95, '√¥ng': 96, 'ƒÉ': 97, 'ƒë√∫ng': 98, 'ƒë√∫ng_kh√¥ng': 99, '∆°': 100, '∆∞': 101}\n",
        "\n",
        "# T·∫°o dict index -> label\n",
        "index_to_label = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "# -----------------------\n",
        "# FastAPI\n",
        "# -----------------------\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],      # Cho ph√©p t·∫•t c·∫£ domain (frontend test)\n",
        "    allow_methods=[\"*\"],      # Cho ph√©p t·∫•t c·∫£ HTTP methods\n",
        "    allow_headers=[\"*\"],      # Cho ph√©p t·∫•t c·∫£ header\n",
        "    allow_credentials=False   # B·∫Øt bu·ªôc False n·∫øu allow_origins=\"*\"\n",
        ")\n",
        "\n",
        "class SequenceInput(BaseModel):\n",
        "    data: list  # list c√°c frame base64\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(input_data: SequenceInput):\n",
        "    # Tr√≠ch feature cho t·ª´ng frame\n",
        "    feature_list = []\n",
        "    for b64 in input_data.data:\n",
        "        frame = decode_base64_to_frame(b64)\n",
        "        feat = extract_full_features(frame)  # 2192-dim\n",
        "        feature_list.append(feat)\n",
        "\n",
        "    feature_array = interpolate_keypoints(feature_list, target_len=60)  # (60, 2192)\n",
        "\n",
        "    X = torch.tensor(feature_array, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 60, 2192)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(X)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        pred_idx = torch.argmax(probs, dim=1).item()   # index\n",
        "        confidence = probs.max().item()\n",
        "\n",
        "    pred_label = index_to_label[pred_idx]  # label th·∫≠t s·ª± theo mapping l√∫c train\n",
        "\n",
        "    return {\"prediction\": pred_label, \"confidence\": float(confidence)}\n",
        "\n",
        "public_url = ngrok.connect(8000, bind_tls=True)\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "\n",
        "# C·∫•u h√¨nh server Uvicorn\n",
        "config = uvicorn.Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Ch·∫°y server\n",
        "asyncio.get_event_loop().create_task(server.serve())\n",
        "print(\"‚úÖ FastAPI server is running...\")"
      ],
      "metadata": {
        "id": "Z1mRgJZUeucw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60511b05-daa8-40ea-9752-7a0d919b0597"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Public URL: NgrokTunnel: \"https://reasonably-protandrous-lexie.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
            "‚úÖ FastAPI server is running...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-z7HfTLsPUsl"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}